{"cells":[{"cell_type":"code","source":["# Graph Analysis"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport datetime as dt\npd.set_option('display.max_rows', 500)\n\n# Enable Arrow-based columnar data transfers\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n\nres_folder = '/dbfs/user/yao/graph_research/geography'\ngeo_code = 'philly'\n# cbsa_code = 37980"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["end_date = '2020-07-31'\ndate_fmt = '%Y-%m-%d'\nlast_month_end = (dt.datetime.strptime(end_date, date_fmt).replace(day=1)+dt.timedelta(days=-1)).strftime(date_fmt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: &#39;2020-06-30&#39;</div>"]}}],"execution_count":3},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW philly_year AS\n(\n\n    select year(ao.purchase_date_pt) year, count(*) vol, count(distinct ao.shop_id) unique_shops, count(distinct coalesce(ao.customer_id, ao.customer_phone)) unique_customers, sum(ao.gmv) total_gmv      \n    from aggregates.all_orders ao\n    join aggregates.dim_shops ds on ao.shop_id = ds.shop_id    \n    join dimension.zipcode_cbsa_regions cbsa on ds.postal_code = cbsa.zip\n    where cbsa.cbsa_code = 37980\n    and (ds.is_live = True or ds.is_temporarily_suspended = True) and ds.shop_type in ('freemium', 'partner')\n    and ds.ds_pt = '2020-07-31'\n--   and ao.purchase_date_pt between '2019-03-01' and '2020-02-29'\n    and ao.is_all_menus_order = false\n    and ao.is_successful = true\n    group by 1\n    order by 1\n);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":4},{"cell_type":"code","source":["df = spark.table('philly_year').toPandas()\ndf"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>vol</th>\n      <th>unique_shops</th>\n      <th>unique_customers</th>\n      <th>total_gmv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010</td>\n      <td>29</td>\n      <td>4</td>\n      <td>27</td>\n      <td>688.97</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011</td>\n      <td>987</td>\n      <td>16</td>\n      <td>673</td>\n      <td>28820.98</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2012</td>\n      <td>4436</td>\n      <td>33</td>\n      <td>2723</td>\n      <td>121529.42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2013</td>\n      <td>18933</td>\n      <td>65</td>\n      <td>11522</td>\n      <td>522877.61</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2014</td>\n      <td>44524</td>\n      <td>91</td>\n      <td>26250</td>\n      <td>1249889.82</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2015</td>\n      <td>73211</td>\n      <td>187</td>\n      <td>42738</td>\n      <td>2022293.96</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2016</td>\n      <td>130867</td>\n      <td>234</td>\n      <td>72507</td>\n      <td>3659944.49</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2017</td>\n      <td>253707</td>\n      <td>347</td>\n      <td>126654</td>\n      <td>7132773.49</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2018</td>\n      <td>493658</td>\n      <td>551</td>\n      <td>213790</td>\n      <td>14197085.07</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2019</td>\n      <td>913109</td>\n      <td>783</td>\n      <td>338287</td>\n      <td>26957804.34</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2020</td>\n      <td>878764</td>\n      <td>902</td>\n      <td>333168</td>\n      <td>30044473.48</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["import datetime as dt\nfrom dateutil.relativedelta import relativedelta\n\nyear_start = '2015-01-01'\ndate_fmt = '%Y-%m-%d'\ndf_arr = []\nwhile year_start < '2020-01-02':\n    year_end = (dt.datetime.strptime(year_start, date_fmt)+relativedelta(years=1)+relativedelta(days=-1)).strftime(date_fmt)\n    if year_start[:4] == '2020':\n        year_end = '2020-07-31'\n    if year_start[:4] == '2015':\n        year_end = '2016-12-31'\n    print(year_start, year_end)\n    sql_str = f\"\"\"\n        CREATE OR REPLACE TEMPORARY VIEW philly_year AS\n        (\n            select year(ao.purchase_date_pt) year, count(*) vol, count(distinct ao.shop_id) unique_shops, count(distinct coalesce(ao.customer_id, ao.customer_phone)) unique_customers, sum(ao.gmv) total_gmv      \n            from aggregates.all_orders ao\n            join aggregates.dim_shops ds on ao.shop_id = ds.shop_id    \n            join dimension.zipcode_cbsa_regions cbsa on ds.postal_code = cbsa.zip\n            where cbsa.cbsa_code = 37980\n            and (ds.is_live = True or ds.is_temporarily_suspended = True) and ds.shop_type in ('freemium', 'partner')\n            and ds.ds_pt = '{year_end}'\n            and ao.purchase_date_pt between '{year_start}' and '{year_end}'\n            and ao.is_all_menus_order = false\n            and ao.is_successful = true\n            group by 1\n        )\n    \"\"\"  \n    _ = spark.sql(sql_str) \n    df_arr.append(spark.table('philly_year').toPandas())\n    year_start = (dt.datetime.strptime(year_end, date_fmt)+relativedelta(days=1)).strftime(date_fmt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2015-01-01 2016-12-31\n2017-01-01 2017-12-31\n2018-01-01 2018-12-31\n2019-01-01 2019-12-31\n2020-01-01 2020-07-31\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["import pandas as pd\npd.concat(df_arr)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>vol</th>\n      <th>unique_shops</th>\n      <th>unique_customers</th>\n      <th>total_gmv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015</td>\n      <td>89569</td>\n      <td>233</td>\n      <td>52681</td>\n      <td>2464952.02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016</td>\n      <td>155891</td>\n      <td>323</td>\n      <td>86878</td>\n      <td>4338397.01</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2017</td>\n      <td>297390</td>\n      <td>475</td>\n      <td>149809</td>\n      <td>8336390.28</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2018</td>\n      <td>559551</td>\n      <td>699</td>\n      <td>244243</td>\n      <td>16059710.40</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2019</td>\n      <td>933559</td>\n      <td>836</td>\n      <td>347029</td>\n      <td>27520609.72</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020</td>\n      <td>834678</td>\n      <td>899</td>\n      <td>320345</td>\n      <td>28473463.28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["sql_str = f\"\"\"\nCREATE OR REPLACE TEMPORARY VIEW shop_consumer_geo AS\n(\n    select ao.purchase_date_pt, ao.shop_id, ds.postal_code shop_zip, ds.shop_lat, ds.shop_lng, coalesce(ao.customer_id, ao.customer_phone) customer_id, ao.customer_zip, ao.gmv,\n    case when order_channel like '%app' then 'app' \n    when order_channel = 'call' then 'phone' \n    else 'other' end as channel      \n    from aggregates.all_orders ao\n    join aggregates.dim_shops ds on ao.shop_id = ds.shop_id    \n    join dimension.zipcode_cbsa_regions cbsa on ds.postal_code = cbsa.zip\n    where cbsa.cbsa_code = 37980\n    and (ds.is_live = True or ds.is_temporarily_suspended = True) and ds.shop_type in ('freemium', 'partner')\n    and ds.ds_pt = '2020-02-29'\n    and ao.purchase_date_pt between '2019-03-01' and '2020-02-29'\n    and ao.is_all_menus_order = false\n    and ao.is_successful = true\n)\n\"\"\""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom graphframes import *\nfrom time import time\nfrom itertools import combinations\nimport geopy.distance\nimport math\n\n\npd.set_option('display.max_rows', 500)\n\n# Enable Arrow-based columnar data transfers\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n\nres_folder = '/dbfs/user/yao/graph_research/time'\ngeo_code = 'philly'\ncbsa_code = 37980\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["def construct_shop_edges():\n    geo_df = spark.table('philly_year').toPandas()\n    geo_shops = geo_df.groupby(['shop_id', 'shop_lat', 'shop_lng']).agg({'customer_id': lambda x: x.nunique(), 'gmv': np.sum}).reset_index().rename(columns={'customer_id':'n_customer', 'gmv':'total_gmv'})\n    print(f'# shops: {geo_shops.shape[0]}')\n\n    geo_df = geo_df[~geo_df['customer_id'].isnull()]\n    app_users = geo_df[geo_df['channel']=='app'][['customer_id']].drop_duplicates()\n\n    shop_ids = geo_df['shop_id'].unique()\n    customer_ids = geo_df['customer_id'].unique()\n    shop_nodes = pd.DataFrame({'id':shop_ids, 'type': ['shop']*len(shop_ids)})\n    customer_nodes = pd.DataFrame({'id':customer_ids, 'type': ['customer']*len(customer_ids)})\n    edges = geo_df.rename(columns={'shop_id':'src', 'customer_id':'dst'})\n\n    # find multi-shop consumers\n\n    count_shops = edges.groupby('dst')['src'].agg(lambda x: x.nunique()).reset_index().rename(columns={'src':'n_shops'}).sort_values('n_shops', ascending=False)\n    multi_shops_c = count_shops[count_shops['n_shops']>1]\n    multi_shops_c_edges = edges.merge(multi_shops_c[['dst']], on='dst')\n    multi_c_shop = multi_shops_c_edges[['dst', 'src', 'shop_lat', 'shop_lng','gmv']].groupby(['dst', 'src', 'shop_lat', 'shop_lng',]).agg(len).rename(columns={'gmv':'one_shop_count'}).reset_index()\n\n    # repeat shop-shop edge by log2(N) where N is the # of purchases of the connecting consumer.\n    \n    by_c_count = edges[['dst', 'src']].groupby('dst').count().rename(columns={'src':'total_count'}).reset_index()\n    by_c_count['n_rep'] = by_c_count['total_count'].apply(lambda x: round(np.log2(x)))\n    multi_c = by_c_count.merge(multi_shops_c[['dst']], on='dst')\n    multi_c['phone']=multi_c['dst'].apply(lambda x: x[0]=='+')\n    multi_c = multi_c.merge(app_users, left_on='dst', right_on='customer_id', how='left', indicator=True)\n    multi_c['app']=multi_c['_merge'].apply(lambda x: 1 if x=='both' else 0)\n    n_app_multi = multi_c['app'].sum()\n    n_phone_multi = multi_c['phone'].sum()\n    app_vol = edges.merge(multi_c[multi_c['app']==1]).shape[0]\n    phone_vol = edges.merge(multi_c[multi_c['phone']==1]).shape[0]\n    print(f'# of multi_shop user: {len(multi_c)}, # of multi_shop app user: {n_app_multi}, # of multi_shop phone user: {n_phone_multi}')\n\n    consumers = edges[['dst']].drop_duplicates()\n    consumers['phone']=consumers['dst'].apply(lambda x: x[0]=='+')\n    n_phone_users = consumers['phone'].sum()\n    print( f'# of all users: {consumers.shape[0]}, # of app user: {app_users.shape[0]}, # of phone user: {n_phone_users}')\n    print(f'# of total vol: {len(edges)},# of multi_shop vol: {len(multi_shops_c_edges)}, # of multi_shop app user vol: {app_vol}, # of multi_shop phone user: {phone_vol}')\n\n    rep_map = {}\n    for k, v, isapp in zip(multi_c['dst'], multi_c['n_rep'], multi_c['app']):\n      app_weight = 1.5 if isapp else 1 # app user weight 1.5\n      rep_map[k] = v * app_weight\n\n    s_edges_arr = []\n    dist_arr = []\n    s_count_arr = []\n    count = 0\n    for dst in multi_c['dst']:\n        count+=1\n        if not count % 5000:\n            print(f'processing {count}th consumer')\n        temp=multi_c_shop[multi_c_shop['dst']==dst]\n        s_count_arr.append(len(temp))\n        for el in combinations(temp['src'], 2):\n            [s_edges_arr.append([el[0], el[1]]) for _ in range(int(rep_map[dst]))]\n\n    # compute a few metrics on multi-shop consumers        \n            sa = temp[temp['src']==el[0]]\n            sb = temp[temp['src']==el[1]]\n            dist_arr.append(geopy.distance.geodesic((sa['shop_lat'].tolist()[0], sa['shop_lng'].tolist()[0]), (sb['shop_lat'].tolist()[0], sb['shop_lng'].tolist()[0])).miles)\n    shop_edges = pd.DataFrame(s_edges_arr, columns=['src','dst'])\n    \n    print(f'# shop-c-shop links: {len(dist_arr)}, median dist: {np.median(dist_arr)}, mean dist: {np.mean(dist_arr)}')\n    print(f'unique shop count by multi-shop consumer - median: {np.median(s_count_arr):.1f}, mean: {np.mean(s_count_arr):.1f}')     \n\n    multi_count_arr = []\n    for shop_id in edges['src'].unique():\n        temp = edges[edges['src']==shop_id]\n        multi_temp=temp.merge(multi_shops_c[['dst']], on='dst')\n        multi_count_arr.append((shop_id, multi_temp['dst'].nunique(), temp['dst'].nunique()))\n\n    print('silos: ', [el[0] for el in multi_count_arr if el[1]==0])\n    multi_consumer_count = [el[1] for el in multi_count_arr if el[1]>0]\n    multi_consumer_pct = [el[1]/el[2] for el in multi_count_arr if el[1]>0]\n    print(f'unique multi-shop consumer count by store - median: {np.median(multi_consumer_count):.1f}, mean: {np.mean(multi_consumer_count):.1f}')\n    print(f'unique multi-shop consumer pct by store - median: {np.median(multi_consumer_pct)*100:.1f}, mean: {np.mean(multi_consumer_pct)*100:.1f}')\n    \n    return geo_df, shop_nodes, shop_edges\n\ndef label_propogation(shop_nodes, shop_edges):\n    # construct shop-to-shop graph            \n    vertices_df = spark.createDataFrame(shop_nodes)\n    edges_df = spark.createDataFrame(shop_edges)\n    print(f'shop-shop graph - nodes: {shop_nodes.shape[0]}, edges: {shop_edges.shape[0]}')\n    shop_graph = GraphFrame(vertices_df, edges_df)        \n\n    # community detection using label propogation\n    start = time()\n    communities = shop_graph.labelPropagation(maxIter=10)\n    print(time()-start)\n    print(f\"There are {communities.select('label').distinct().count()} communities in sample graph.\")\n\n    community_df = communities.toPandas()\n    community_df.to_csv(f'{res_folder}/community_{geo_code}_{year_end[:4]}.csv')\n\n    size_arr = []\n    for label in community_df['label'].unique():\n        sub = community_df[community_df['label']==label]\n        size_arr.append([sub.shape[0], label])\n\n    size_df = pd.DataFrame(size_arr, columns=['size', 'label'])\n    size_df.sort_values('size', ascending=False, inplace=True)\n    return community_df, size_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["def community_summary(label, community_df, shop_edges):\n    sub_df = community_df[community_df['label']==label].copy()\n    vertices_sub = sub_df[['id']]\n    edges_l = pd.merge(shop_edges, vertices_sub, left_on='src', right_on='id')\n    edges_sub_all = pd.merge(edges_l, vertices_sub, left_on='dst', right_on='id', how='left', indicator=True)\n    edges_sub_intra = edges_sub_all[edges_sub_all['_merge']=='both']\n    edges_sub_inter = edges_sub_all[edges_sub_all['_merge']!='both']\n\n    multi_count_arr_sub = []\n    for shop_id in edges_sub_intra['src'].unique():\n        temp = edges[edges['src']==shop_id]\n        multi_temp=temp.merge(multi_shops_c[['dst']], on='dst')\n        multi_count_arr_sub.append((shop_id, multi_temp['dst'].nunique(), temp['dst'].nunique()))\n\n    multi_consumer_count_sub = [el[1] for el in multi_count_arr_sub if el[1]>0]\n    multi_consumer_pct_sub = [el[1]/el[2] for el in multi_count_arr_sub if el[1]>0]\n\n    return [label, vertices_sub.shape[0], edges_sub_intra.shape[0], edges_sub_inter.shape[0], edges_sub_intra.drop_duplicates().shape[0],edges_sub_inter.drop_duplicates().shape[0], np.median(multi_consumer_pct_sub)]\n\n\ndef get_metrics(geo_df, community_df, size_df, shop_edges, year_end):\n    # business metrics summary\n    geo_user = geo_df[['customer_id']].drop_duplicates().merge(app_users, on='customer_id', how='left', indicator=True)\n    geo_user['app']=geo_user['_merge'].apply(lambda x: 1 if x=='both' else 0)\n    geo_shop_app = geo_df[['shop_id', 'customer_id']].drop_duplicates().merge(geo_user, on='customer_id').groupby('shop_id').agg({'customer_id':len, 'app': sum}).reset_index()\n    geo_shop_app['app_user_ratio'] = geo_shop_app['app']/geo_shop_app['customer_id']\n    geo_shops = geo_shops.merge(geo_shop_app, on='shop_id')\n\n    metrics_arr = []\n    for i, label in enumerate(size_df.label):  \n        sub = community_df[community_df['label']==label]\n        sub = sub.merge(geo_shops, left_on='id', right_on='shop_id')\n        metrics_arr.append([label, sub['total_gmv'].mean(), sub['app_user_ratio'].mean()])\n\n    business_df = pd.DataFrame(metrics_arr, columns=['label','gmv_mean', 'app_user_ratio_mean'])\n\n    # radius of community\n    sub_arr = []\n\n    for i, label in enumerate(size_df.label):  \n        sub = community_df[community_df['label']==label]\n        sub = sub.merge(geo_shops, left_on='id', right_on='shop_id')\n        center_lat, center_lng = sub['shop_lat'].median(), sub['shop_lng'].median()\n        sub['dist'] = sub.apply(lambda x: geopy.distance.geodesic((x['shop_lat'], x['shop_lng']), (center_lat, center_lng)).miles, axis=1)\n        sub_arr.append(sub)\n\n    gmap_final = pd.concat(sub_arr)\n    radius_df = gmap_final.groupby(['label'])['dist'].agg(lambda x: np.percentile(x, 80)).reset_index().rename(columns={'dist':'radius'}).merge(size_df, on='label').sort_values('size', ascending=False)\n\n    # community_statistics\n    size_df = size_df[size_df['size']>1].copy()\n    temp = size_df.merge(community_df, on='label')\n    summary_arr = []\n    for i, label in enumerate(temp['label'].unique()):\n        res = community_summary(label, community_df, shop_edges, i) \n        summary_arr.append(res)\n    summary_df = pd.DataFrame(summary_arr, columns=['label', 'nodes', 'in_edges', 'out_edges', 'u_in_edges', 'u_out_edges', 'mc_pct'])\n    \n    # app retention & new users\n    year_end_minus_one = (dt.datetime.strptime(year_end, date_fmt)+relativedelta(months=-1)).strftime(date_fmt)\n    app_df = geo_df[geo_df['channel']=='app']\n    last_month = app_df[(app_df['purchase_date_pt']>year_end_minus_one)&(app_df['purchase_date_pt']<=year_end)].groupby('shop_id')['customer_id'].agg(lambda x: set(x)).reset_index().rename(columns={'customer_id':'last_id'})\n    this_month = app_df[app_df['purchase_date_pt']>year_end_minus_one].groupby('shop_id')['customer_id'].agg(lambda x: set(x)).reset_index()\n\n    retention = last_month.merge(this_month, on='shop_id', how='left')\n    retention['retention'] = retention.apply(lambda x: len(x.last_id&x.customer_id)/len(x.last_id) if type(x.customer_id)==set else 0, axis=1)\n\n    prior_months = app_df[app_df['purchase_date_pt']<=year_end_minus_one].groupby('shop_id')['customer_id'].agg(lambda x: set(x)).reset_index().rename(columns={'customer_id':'prior_id'})\n\n    new = prior_months.merge(this_month, on='shop_id', how='left')\n    new['new_pct'] = new.apply(lambda x: len(x.customer_id-x.prior_id)/len(x.customer_id) if type(x.customer_id)==set else 0,axis=1)\n    new['new_count'] = new.apply(lambda x: len(x.customer_id-x.prior_id) if type(x.customer_id)==set else 0,axis=1)\n    \n    shop_retention = community_df.merge(retention, left_on='id', right_on='shop_id', how='left')\n    community_retention = shop_retention[~shop_retention['retention'].isnull()].groupby('label')['retention'].agg(np.mean).reset_index()\n\n    shop_new = community_df.merge(new, left_on='id', right_on='shop_id', how='left')\n    community_new_pct = shop_new[~shop_new['new_pct'].isnull()].groupby('label')['new_pct'].agg(np.mean).reset_index()\n    community_new_count = shop_new[~shop_new['new_count'].isnull()].groupby('label')['new_count'].agg(np.mean).reset_index()\n    \n    # combine all metrics\n    performace_metrics = radius_df.merge(summary_df, on='label').merge(business_df, on='label').merge(community_retention, on='label', how='left').merge(community_new_pct, on='label', how='left').merge(community_new_count, on='label', how='left')\n    performace_metrics.to_csv(f'{res_folder}/metrics_{geo_code}_{year_end[:4]}.csv')\n    \ndef get_map_markers(size_df):\n    colors = ['red', 'blue', 'yellow', 'green', 'purple', 'teal', 'gray', 'olive', 'maroon', 'navy', 'black', 'white']\n    mshapes = ['circle', 'square', 'marker']\n\n    map_str_arr = []\n    sub_arr = []\n    top_communities = size_df[size_df['size']>=10] \n    top_labels = size_df['label'].tolist()[:36] if len(top_communities)>36 else top_communities.label\n    for i, label in enumerate(top_labels):  \n        sub = community_df[community_df['label']==label]\n        sub = sub.merge(geo_shops, left_on='id', right_on='shop_id')\n        center_lat, center_lng = sub['shop_lat'].median(), sub['shop_lng'].median()\n        sub['dist'] = sub.apply(lambda x: geopy.distance.geodesic((x['shop_lat'], x['shop_lng']), (center_lat, center_lng)).miles, axis=1)\n\n        color = colors[i%len(colors)]\n        mshape_arr = [mshapes[i//len(colors)]]*len(sub)\n        sub = pd.concat([sub, pd.DataFrame({'mshape': mshape_arr})], axis=1) \n        sub['map_str'] = sub.apply(lambda x: str(x.shop_lat)+','+str(x.shop_lng)+','+color+','+x.mshape+',', axis=1)\n        map_str_arr.extend(sub['map_str'].tolist())\n        sub_arr.append(sub)\n\n    gmap_final = pd.concat(sub_arr)\n    gmap_final.to_csv(f'{res_folder}/gmap_{geo_code}_{year_end[:4]}.csv', index=False)    \n    \ndef process_time_window(year_end):\n    geo_df, shop_nodes, shop_edges = construct_shop_edges()\n    community_df, size_df = label_propogation(shop_nodes, shop_edges)\n    get_metrics(geo_df, community_df, size_df, shop_edges, year_end)\n    get_map_markers(size_df)      "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["year_start = '2015-01-01'\ndate_fmt = '%Y-%m-%d'\ndf_arr = []\nwhile year_start < '2015-01-02':\n    year_end = (dt.datetime.strptime(year_start, date_fmt)+relativedelta(years=1)+relativedelta(days=-1)).strftime(date_fmt)\n    ds_end = year_end\n    if year_start[:4] == '2020':\n        year_end = '2020-07-31'\n    if year_start[:4] == '2015':\n        ds_end = '2016-12-31'\n    print(year_start, year_end)\n    sql_str = f\"\"\"\n        CREATE OR REPLACE TEMPORARY VIEW philly_year AS\n        (\n            select ao.purchase_date_pt, ao.shop_id, ds.postal_code shop_zip, ds.shop_lat, ds.shop_lng, coalesce(ao.customer_id, ao.customer_phone) customer_id, ao.customer_zip, ao.gmv,\n            case when order_channel like '%app' then 'app' \n                when order_channel = 'call' then 'phone' \n                else 'other' end as channel      \n            from aggregates.all_orders ao\n            join aggregates.dim_shops ds on ao.shop_id = ds.shop_id    \n            join dimension.zipcode_cbsa_regions cbsa on ds.postal_code = cbsa.zip\n            where cbsa.cbsa_code = {cbsa_code}\n            and (ds.is_live = True or ds.is_temporarily_suspended = True) and ds.shop_type in ('freemium', 'partner')\n            and ds.ds_pt = '{ds_end}'\n            and ao.purchase_date_pt between '{year_start}' and '{year_end}'\n            and ao.is_all_menus_order = false\n            and ao.is_successful = true\n        )\n    \"\"\"  \n    _ = spark.sql(sql_str)    \n    process_time_window(year_end)\n    year_start = (dt.datetime.strptime(year_end, date_fmt)+relativedelta(days=1)).strftime(date_fmt)"],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"graph_research_philly","notebookId":78477816597359},"nbformat":4,"nbformat_minor":0}